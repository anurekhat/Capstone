{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study of Lending Club loans\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Lending Club is an online credit marketplace, facilitating personal loans, business loans, and financing for elective medical procedures. Lending Club has provided datasets that contain complete loan data for all loans issued through the specific time period stated, including the current loan status (Current, Late, Fully Paid, etc.) , latest payment information and the financial and credit rating information about the loan applicant collected by Lending Club.\n",
    "\n",
    "## Objective\n",
    "\n",
    " The Capstone project aims to learn the payment patterns of applicants and build a predictive model that will predict the repayment class of the loan. For this study, the loan repayment type has been classified into three as follows\n",
    " \n",
    "  - **Good loans** - The installments will be always on time and the loan will be fully paid on or before the end of loan term.\n",
    "  - **Bad loans** - The loan will end up in Charge Off and LendingClub will end up in spending for collection and recoveries process.\n",
    "  - **Delinquent loans** - The loan installments will be delayed one or more times but not likely to end up in charge off.\n",
    "  \n",
    "## Data\n",
    "\n",
    " We will be considering the LendingClub dataset for the year 2014. At a high level, the dataset contains the below attribute groups. \n",
    " \n",
    "  - *Applicant Identity* - Basic Identity information about the applicant like home state, employment length, home-ownership type etc\n",
    "  - *Applicant Income* - The reported annual income as provided by the applicant and the verification status\n",
    "  - *Loan* - Attributes directly related to the loan like the loan purpose, amount, interest rate, status, payment of various fees etc\n",
    "  - *Credit History* - Attributes collected about the applicant's credit history like no. of bankcard accounts, mortgage accounts, credit balances, debt to income ratio etc\n",
    "  - *Delinquency History* - Attributes collected about the applicant's previous delinquent payments like no. of chargeoffs / collections, no. of accounts past due payment etc.\n",
    "  \n",
    "  \n",
    "  - The *loan_status* attribute can be one among the following.\n",
    "      - 'Fully Paid'\n",
    "      - 'Current'\n",
    "      - 'In Grace Period'\n",
    "      - 'Late (16-30 days)'\n",
    "      - 'Late (31-120 days)'\n",
    "      - 'Default'\n",
    "      - 'Charged Off'\n",
    "  \n",
    "    The detailed list of attributes can be viewed [ here ] ('https://github.com/anurekhat/Capstone/blob/master/LendingClub/LCDataDictionary2014.xlsx '). \n",
    "  \n",
    "\n",
    "###  Data Clean ups, filtering and transformations\n",
    " \n",
    " \n",
    "  - The below attributes have been dropped for further analysis as they are manual text entries that serves more as just informational. \n",
    "      - *emp_title*\n",
    "      - *desc*\n",
    "      - *title*\n",
    "      \n",
    "      \n",
    "  - There are two available terms for loans - 36 months and 60 months. For this study, we consider only the 36 months term loans as the 60 months term loans initiated in 2014 will be mostly still running as on 2017 and including them may skew the features of 'Charged Off' loans. \n",
    "  \n",
    "  \n",
    "  - The loan records with *total_rec_late_fee* as a non-zero value have been marked as delinquent. i.e If the applicant has ever paid a late fee, that loan is marked as delinquent. Please see the explanation on **Classification** below\n",
    "  \n",
    "  - The below date attributes have been converted to the appropriate duration attribute, considering Apr 2017 as the reference date\n",
    "      - *issue_d* converted to *mo_sin_loan_funded* (months since loan was funded)\n",
    "      - *earliest_cr_line* converted to *mo_sin_earliest_cr_line* (months since the earliest credit line was opened)\n",
    "      - *last_pymnt_d* converted to *mo_sin_last_pymnt* (months since the last payment)\n",
    "      - *last_credit_pull_d* converted to *mo_sin_last_credit_pull* (months since the last credit pull by LC)\n",
    "      \n",
    "  - **Imputing Missing Values**\n",
    "      - *last_pymnt_d* (*mo_sin_last_pymnt*) : This attribute is expected to be null when there has not been any installments that were paid. The date is set to May 2017 so that the duration attribute holds the value -1 and these records are considered separate by the model estimator\n",
    "      - *last_credit_pull_d* (*mo_sin_last_credit_pull*) : This attribute is expected to be null when LC has not pulled the credit info of the applicant. The date is set to May 2017 so that the duration attribute holds the value -1 and these records are considered separate by the model estimator\n",
    "      - *mths_since_last_delinq* : This attribute is expected to be null when the applicant has no prior delinquencies. Imputing these to -1 to set these apart\n",
    "      - *mths_since_last_record* : This attribute is expected to be null when the applicant has no public records. Imputing these to -1 to set these apart\n",
    "      - *mths_since_last_major_derog* : This attribute is expected to be null when the applicant has no previous derogatory records. Imputing these to -1 to set these apart\n",
    "      - *mo_sin_old_il_acct* : This attribute is expected to be null when the applicant has no bank installment accounts. Imputing these to -1 to set these apart\n",
    "      - *mths_since_recent_bc_dlq* : This attribute is expected to be null when there's no previous bankcard delinquency. Imputing these to -1 to set these apart\n",
    "      - *mths_since_recent_inq* : This attribute is expected to be null when there's no previous inquiries. Imputing these to -1 to set these apart\n",
    "      - *mths_since_recent_revol_delinq* : This attribute is expected to be null when there's no previous revolving account delinquencies. Imputing these to -1 to set these apart\n",
    "      - *num_tl_120dpd_2m* : This attribute is expected to be null when there's no accounts currently 120 days past due. Imputing these to 0\n",
    "      - *mths_since_recent_bc* : This attribute is expected to be null when info not available. Imputing these to the median\n",
    "      - *bc_open_to_buy* : This attribute is expected to be null when info is not available. Imputing these to the median\n",
    "      - *bc_util* : This attribute is expected to be null when info is not available. Imputing these to the median\n",
    "      - *avg_cur_bal* : This attribute is expected to be null when info is not available. Imputing these to the median\n",
    "      - *percent_bc_gt_75* : This attribute is expected to be null when info is not available. Imputing these to the median\n",
    "      - *revol_util* : This attribute is expected to be null when info is not available. Imputing these to the median\n",
    "      \n",
    "  - **Categorical features** \n",
    "      Categorical features have been converted to the 'One Hot Encoding' format using pandas `get_Dummies()` method\n",
    "      \n",
    "  - The following attributes are not being considered as the 'X' independent variables in the model as these are not expected to impact the outcome variable of loan_class\n",
    "      - *term* : As it will be always 36 months\n",
    "      - *zip_code* : The 3 digit coded value is not expected to impact outcome\n",
    "      - *loan_status* : This is more part of the outcome than an input variable\n",
    "      - *application_type* : As it will be always 'INDIVIDUAL'\n",
    "      - *id* : Identifier \n",
    "      - *next_pymnt_d* : Not expected to impact outcome\n",
    "\n",
    "      \n",
    "      \n",
    "#### Classification\n",
    "   - All loan records where *loan_status* is 'Fully Paid' or 'Current' and *total_rec_late_fee* is zero are classified as **Good**\n",
    "      \n",
    "      \n",
    "   - All loan records with *loan_status* 'Charged Off' are classified as **Bad**\n",
    "      \n",
    "      \n",
    "   - All loan records that have a non-zero value for *total_rec_late_fee* or loan_status is one among { 'In Grace Period', 'Late (16-30 days)', 'Late (31-120 days)', 'Default'} is classified as **Delinquent**. Note that there could be some >Charged Off loans that could have been also delinquent. The Charged off status will take priority in such cases and those loans will be classified as **Bad** \n",
    "      \n",
    "  \n",
    "## Methodology\n",
    "\n",
    "\n",
    "The cleaned up dataset has been divided into training (70%) and test (30%) datasets. Models with the below algorithm have been trained and tested with the dataset.\n",
    "\n",
    "    - Logistic Regression\n",
    "    - Linear Support Vector Classifier (LinearSVC)\n",
    "    - Random Forest Classifier\n",
    "    - XG Boost\n",
    "    \n",
    "  The below table summarizes the scores from the different models\n",
    "  \n",
    "\n",
    "Algorithm | Score | Learning | Test\n",
    "--- | --- | --- | --- \n",
    "Logistic Regression| Accuracy | 0.9608 | 0.9612\n",
    "Logistic Regression| F1 |  | 0.9537\n",
    "LinearSVC| Accuracy| 0.9813|0.9824\n",
    "LinearSVC| F1| |0.9800\n",
    "RandomForest| Accuracy |0.9991 |0.9811\n",
    "RandomForest|F1| |0.9792\n",
    "xgBoost| Accuracy|0.9887 |0.9878\n",
    "xgBoost| F1 | |0.9871\n",
    "\n",
    "\n",
    "## Testing on 2012-2013 dataset\n",
    "\n",
    "The XG Boost model was found to have the better scores based on the results from the 2014 test dataset. The LendingClub's 2012-2013 dataset has been used to further test the xgboost model. The results are as below.\n",
    "\n",
    "    - Accuracy score :  0.974159099333\n",
    "    - F1 score :  0.975060440745\n",
    "\n",
    " Class|   F1 |  Precision |   Recall | Total\n",
    "          ---| --- | --- | --- | ---\n",
    "        Bad | 0.907538 |   0.837017 | 0.991035 | 11043\n",
    "        Delinquent |  0.955626  | 0.991776 | 0.922018 |  1308\n",
    "        Good|  0.985482 |  0.998751 | 0.972562 | 73985\n",
    "\n",
    "    - Confusion Matrix\n",
    "\n",
    "    [[10944    10    89]\n",
    " \n",
    "     [  101  1206     1]\n",
    " \n",
    "     [ 2030     0 71955]]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "import statsmodels.stats.weightstats as wst\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the dataset.\n",
    "\n",
    "# Define a function for string % convertion to a float ratio\n",
    "def p2f(x):\n",
    "    perc = None\n",
    "    if (x != None) :\n",
    "        if (type(x) is str) :\n",
    "            x = x.strip('%')\n",
    "        perc = float(x)/100\n",
    "    return perc\n",
    "\n",
    "loan_data = pd.read_csv('./LoanStats2014.csv', skiprows=1, skipfooter=2, parse_dates = [15, 26, 45,47, 48], \n",
    "                        infer_datetime_format = True, engine = 'python', converters = {'int_rate' : p2f, 'revol_util' : p2f})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removing the attribute columns that have all null values. Only the attributes listed in the above DataDictionary will remain.\n",
    "loan_data.dropna(axis = 1, how = 'all', inplace= True)\n",
    "\n",
    "#Since id values are not provided in the dataset, create id variable from the index.\n",
    "loan_data['id'] = loan_data.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loan_data['term_mnths'] = loan_data.term.str.strip('months').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235629"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loan_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162570"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering out the 36 months term loans\n",
    "loan_data = loan_data[loan_data.term_mnths == 36]\n",
    "len(loan_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dropping textual columns from the dataset\n",
    "loan_data.drop(['emp_title', 'desc', 'title'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Marking loans that had at least one late payment\n",
    "loan_data['late_fee_rec_indicator'] = (loan_data.total_rec_late_fee > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loans across different loan statuses\n",
    "#sns.countplot(x = 'loan_status', hue = 'late_fee_rec_indicator', data = loan_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classify the loans as 'Good' ,'Bad', 'Delinquent'\n",
    "\n",
    "def classify_loan(row) :\n",
    "    val = ''\n",
    "    if ((row.loan_status in ['Fully Paid', 'Current']) & (row.late_fee_rec_indicator == False) ) :\n",
    "        val = 'Good'\n",
    "    elif row.loan_status in ['Charged Off'] :\n",
    "        val = 'Bad'\n",
    "    elif ( (row.loan_status not in ['Fully Paid', 'Current', 'Charged Off']) | \n",
    "         row.late_fee_rec_indicator == True) :\n",
    "        val = 'Delinquent'\n",
    "    return val\n",
    "\n",
    "loan_data['loan_class'] = loan_data.apply(classify_loan, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bad</th>\n",
       "      <td>19380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delinquent</th>\n",
       "      <td>5018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Good</th>\n",
       "      <td>138172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "loan_class        \n",
       "Bad          19380\n",
       "Delinquent    5018\n",
       "Good        138172"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(loan_data.groupby('loan_class').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame( loan_data.groupby(['loan_class', 'loan_status','late_fee_rec_indicator']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform date fields into appropriate duration units\n",
    "from datetime import datetime\n",
    "loans = loan_data.copy()\n",
    "ref_datetime = datetime.strptime('2017-04-02', '%Y-%m-%d')\n",
    "# Issue date\n",
    "loans['mo_sin_loan_funded'] = ((ref_datetime - loans['issue_d']) / np.timedelta64(1, 'M')).astype(int)\n",
    "\n",
    "# Earliest credit line\n",
    "loans['mo_sin_earliest_cr_line'] = ((ref_datetime - loans['earliest_cr_line']) / np.timedelta64(1, 'M')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imputing missing values and converting to duration units\n",
    "#Last payment date is null when no installments have been made. Setting the nulls to May 2017, so that the calculated duration\n",
    "# field will have the value -1\n",
    "loans['last_pymnt_d'] = loans['last_pymnt_d'].fillna(pd.to_datetime('2017-05-02'))\n",
    "loans['mo_sin_last_pymnt'] = ((ref_datetime - loans['last_pymnt_d']) / np.timedelta64(1, 'M')).astype(int)\n",
    "\n",
    "#last_credit_pull_d\n",
    "#Setting the nulls to May 2017, so that the calculated duration field will have the value -1\n",
    "loans['last_credit_pull_d'] = loans['last_credit_pull_d'].fillna(pd.to_datetime('2017-05-02'))\n",
    "loans['mo_sin_last_credit_pull'] = ((ref_datetime - loans['last_credit_pull_d']) / np.timedelta64(1, 'M')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imputing missing values\n",
    "\n",
    "#mths_since_last_delinq\n",
    "# this will be null when the applicant has no prior delinquencies. Imputing these to -1 to set these apart.\n",
    "loans['mths_since_last_delinq'] = loans['mths_since_last_delinq'].fillna(-1)\n",
    "\n",
    "#mths_since_last_record\n",
    "# this will be null when the applicant has no public records. Imputing these to -1 to set these apart.\n",
    "loans['mths_since_last_record'] = loans['mths_since_last_record'].fillna(-1)\n",
    "\n",
    "#mths_since_last_major_derog\n",
    "# this will be null when the applicant has no previous derogatory records. Imputing these to -1 to set these apart.\n",
    "loans['mths_since_last_major_derog'] = loans['mths_since_last_major_derog'].fillna(-1)\n",
    "\n",
    "#mo_sin_old_il_acct\n",
    "# this will be null when the applicant has no bank installment accounts. Imputing these to -1 to set these apart.\n",
    "loans['mo_sin_old_il_acct'] = loans['mo_sin_old_il_acct'].fillna(-1)\n",
    "\n",
    "#mths_since_recent_bc_dlq\n",
    "# this will be null when there's no previous bankcard delinquency. Imputing these to -1 to set these apart.\n",
    "loans['mths_since_recent_bc_dlq'] = loans['mths_since_recent_bc_dlq'].fillna(-1)\n",
    "\n",
    "#mths_since_recent_inq\n",
    "# this will be null when there's no previous inquiries. Imputing these to -1 to set these apart.\n",
    "loans['mths_since_recent_inq'] = loans['mths_since_recent_inq'].fillna(-1)\n",
    "\n",
    "#mths_since_recent_revol_delinq\n",
    "# this will be null when there's no previous revolving account delinquencies. Imputing these to -1 to set these apart.\n",
    "loans['mths_since_recent_revol_delinq'] = loans['mths_since_recent_revol_delinq'].fillna(-1)\n",
    "\n",
    "#num_tl_120dpd_2m\n",
    "# this will be null when there's no accounts currently 120 days past due. Imputing these to 0.\n",
    "loans['num_tl_120dpd_2m'] = loans['num_tl_120dpd_2m'].fillna(0)\n",
    "\n",
    "#mths_since_recent_bc\n",
    "# this will be null when info not available. Imputing these to the median\n",
    "loans['mths_since_recent_bc'] = loans['mths_since_recent_bc'].fillna(loans.mths_since_recent_bc.median())\n",
    "\n",
    "#bc_open_to_buy\n",
    "# this will be null when info is not available. Imputing these to the median\n",
    "loans['bc_open_to_buy'] = loans['bc_open_to_buy'].fillna(loans.bc_open_to_buy.median())\n",
    "\n",
    "#bc_util\n",
    "# this will be null when info is not available. Imputing these to the median\n",
    "loans['bc_util'] = loans['bc_util'].fillna(loans.bc_util.median())\n",
    "\n",
    "#avg_cur_bal\n",
    "# this will be null when info is not available. Imputing these to the median\n",
    "loans['avg_cur_bal'] = loans['avg_cur_bal'].fillna(loans.avg_cur_bal.median())\n",
    "\n",
    "#percent_bc_gt_75\n",
    "# this will be null when info is not available. Imputing these to the median\n",
    "loans['percent_bc_gt_75'] = loans['percent_bc_gt_75'].fillna(loans.percent_bc_gt_75.median())\n",
    "\n",
    "#revol_util\n",
    "# this will be null when info is not available. Imputing these to the median\n",
    "loans['revol_util'] = loans['revol_util'].fillna(loans.revol_util.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Drop the now unused columns\n",
    "loans.drop(['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert categorical features to One Hot Encode format using Pandas getDummies\n",
    "categorical_features = ['grade', 'sub_grade', 'home_ownership','emp_length', 'verification_status', 'pymnt_plan', 'purpose', 'addr_state',\n",
    "                       'initial_list_status']\n",
    "loans_with_dummies = pd.get_dummies(columns = categorical_features, data= loans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement logistic regression with Grid Search Cross Validation for tuning hyper parameters\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Identifying X values - Removing all derived columns, date columns, id and constants\n",
    "X = loans_with_dummies.drop(['term', 'zip_code','loan_status', 'application_type','id','term_mnths', 'late_fee_rec_indicator', \n",
    "                             'loan_class', 'next_pymnt_d' ], axis = 1)\n",
    "loans_with_dummies['loan_class'] = loans_with_dummies.loan_class.astype('category')\n",
    "Y = loans_with_dummies.loan_class.cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113799, 201)\n",
      "(113799,)\n",
      "(48771, 201)\n",
      "(48771,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into a training and test set.\n",
    "Xlr, Xtestlr, ylr, ytestlr = train_test_split(X.values, Y.values, test_size = 0.30, random_state = 5 )\n",
    "\n",
    "print(Xlr.shape)\n",
    "print (ylr.shape)\n",
    "\n",
    "print(Xtestlr.shape)\n",
    "print(ytestlr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#parameters = { 'C' : [ 0.1, 1, 10, 100]}\n",
    "parameters = {'C' : [1]}\n",
    "#Using the class_weight parameter as balanced so that the algo adjusts for the less frequent loan classes of bad and delinquent\n",
    "lr = LogisticRegression(class_weight= 'balanced')\n",
    "clf = GridSearchCV(lr, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params {'C': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fit the model on the best model\n",
    "clf.fit(Xlr, ylr)\n",
    "print ('Best Params',clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score from learning dataset 0.960869603424\n",
      "Accuracy score from testing dataset 0.961247462631\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Print the accuracy from the test data.\n",
    "print('Accuracy score from learning dataset',accuracy_score(ylr, clf.predict(Xlr) ))\n",
    "\n",
    "# Print the accuracy from the test data.\n",
    "print('Accuracy score from testing dataset',accuracy_score(ytestlr, clf.predict(Xtestlr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss from learning dataset 0.25464750644\n",
      "Log loss from testing dataset 0.256346470061\n"
     ]
    }
   ],
   "source": [
    "# Log loss\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Print the log loss from the test data.\n",
    "print('Log loss from learning dataset',log_loss(ylr, clf.predict_proba(Xlr)))\n",
    "\n",
    "# Print the log loss from the test data.\n",
    "print('Log loss from testing dataset',log_loss(ytestlr, clf.predict_proba(Xtestlr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learing dataset: \n",
      "\n",
      "         F1  Precision    Recall  Total\n",
      "0  0.994838   0.999926  0.989802  13630\n",
      "1  0.119616   0.214967  0.082862   3536\n",
      "2  0.977231   0.965819  0.988917  96633\n",
      "Test dataset: \n",
      "\n",
      "         F1  Precision    Recall  Total\n",
      "0  0.995896   1.000000  0.991826   5750\n",
      "1  0.126126   0.212121  0.089744   1482\n",
      "2  0.977495   0.967107  0.988108  41539\n"
     ]
    }
   ],
   "source": [
    "# precision recall f1 score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "print ('Learing dataset: \\n')\n",
    "prec, recall, f1, support = precision_recall_fscore_support(ylr, clf.predict(Xlr))\n",
    "prf_lr = pd.DataFrame({'Precision': prec, 'Recall': recall, 'F1': f1, 'Total': support})\n",
    "print (prf_lr)\n",
    "\n",
    "print ('Test dataset: \\n')\n",
    "prec, recall, f1, support = precision_recall_fscore_support(ytestlr, clf.predict(Xtestlr))\n",
    "\n",
    "prf_test = pd.DataFrame({'Precision': prec, 'Recall': recall, 'F1': f1, 'Total': support})\n",
    "print (prf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learing dataset: \n",
      "\n",
      "[[13491     0   139]\n",
      " [    0   293  3243]\n",
      " [    1  1070 95562]]\n",
      "Test dataset: \n",
      "\n",
      "[[ 5703     0    47]\n",
      " [    0   133  1349]\n",
      " [    0   494 41045]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print ('Learing dataset: \\n')\n",
    "print (confusion_matrix(ylr, clf.predict(Xlr)))\n",
    "\n",
    "print ('Test dataset: \\n')\n",
    "print (confusion_matrix(ytestlr, clf.predict(Xtestlr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95379369703207106"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(ytestlr, clf.predict(Xtestlr), average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model\n",
    "\n",
    " Re-using the learning set and test set created earlier using train_test_split (70% | 30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Scale X variables\n",
    "Xlr_svm = preprocessing.scale(Xlr)\n",
    "Xtestlr_svm = preprocessing.scale(Xtestlr)\n",
    "\n",
    "# dual = False as n_samples > n_features\n",
    "#class_weight = balanced so that the algo adjusts for the less frequent loan classes of bad and delinquent\n",
    "#using the default ovr classifier for the multi class outcome\n",
    "#parameters = { 'C': [0.1, 1, 10]}\n",
    "parameters = {'C' : [1]}\n",
    "clf_svm = GridSearchCV(svm.LinearSVC(dual = False, class_weight= 'balanced'), parameters, n_jobs = -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight='balanced', dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "       fit_params={}, iid=True, n_jobs=-1, param_grid={'C': [1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm.fit(Xlr_svm, ylr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>462.778926</td>\n",
       "      <td>0.122827</td>\n",
       "      <td>0.981063</td>\n",
       "      <td>0.981327</td>\n",
       "      <td>1</td>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.981415</td>\n",
       "      <td>0.981058</td>\n",
       "      <td>0.981204</td>\n",
       "      <td>0.98156</td>\n",
       "      <td>0.98057</td>\n",
       "      <td>0.981362</td>\n",
       "      <td>138.169983</td>\n",
       "      <td>0.023181</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_C  \\\n",
       "0     462.778926         0.122827         0.981063          0.981327       1   \n",
       "\n",
       "     params  rank_test_score  split0_test_score  split0_train_score  \\\n",
       "0  {'C': 1}                1           0.981415            0.981058   \n",
       "\n",
       "   split1_test_score  split1_train_score  split2_test_score  \\\n",
       "0           0.981204             0.98156            0.98057   \n",
       "\n",
       "   split2_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "0            0.981362    138.169983        0.023181        0.000359   \n",
       "\n",
       "   std_train_score  \n",
       "0         0.000206  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clf_svm.best_params_)\n",
    "pd.DataFrame(clf_svm.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score from learning dataset 0.981379449732\n",
      "Accuracy score from testing dataset 0.982469090238\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score\n",
    "\n",
    "# Print the accuracy from the test data.\n",
    "print('Accuracy score from learning dataset',accuracy_score(ylr, clf_svm.predict(Xlr_svm) ))\n",
    "\n",
    "# Print the accuracy from the test data.\n",
    "print('Accuracy score from testing dataset',accuracy_score(ytestlr, clf_svm.predict(Xtestlr_svm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learing dataset: \n",
      "\n",
      "         F1  Precision    Recall  Total\n",
      "0  0.999119   1.000000  0.998239  13630\n",
      "1  0.639023   0.815905  0.525170   3536\n",
      "2  0.989124   0.982638  0.995695  96633\n",
      "Test dataset: \n",
      "\n",
      "         F1  Precision    Recall  Total\n",
      "0  0.998868   1.000000  0.997739   5750\n",
      "1  0.632219   0.886724  0.491228   1482\n",
      "2  0.989851   0.981949  0.997882  41539\n"
     ]
    }
   ],
   "source": [
    "print ('Learing dataset: \\n')\n",
    "prec, recall, f1, support = precision_recall_fscore_support(ylr, clf_svm.predict(Xlr_svm))\n",
    "prf_lr = pd.DataFrame({'Precision': prec, 'Recall': recall, 'F1': f1, 'Total': support})\n",
    "print (prf_lr)\n",
    "\n",
    "print ('Test dataset: \\n')\n",
    "prec, recall, f1, support = precision_recall_fscore_support(ytestlr, clf_svm.predict(Xtestlr_svm))\n",
    "\n",
    "prf_test = pd.DataFrame({'Precision': prec, 'Recall': recall, 'F1': f1, 'Total': support})\n",
    "print (prf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learing dataset: \n",
      "\n",
      "[[13606     3    21]\n",
      " [    0  1857  1679]\n",
      " [    0   416 96217]]\n",
      "Test dataset: \n",
      "\n",
      "[[ 5737     5     8]\n",
      " [    0   728   754]\n",
      " [    0    88 41451]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "\n",
    "print ('Learing dataset: \\n')\n",
    "print (confusion_matrix(ylr, clf_svm.predict(Xlr_svm)))\n",
    "\n",
    "print ('Test dataset: \\n')\n",
    "print (confusion_matrix(ytestlr, clf_svm.predict(Xtestlr_svm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9800467734710463"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "f1_score(ytestlr, clf_svm.predict(Xtestlr_svm), average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Forest Model\n",
    "\n",
    " Re-using the learning set and test set created earlier using train_test_split (70% | 30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "parameters = {'n_estimators' : [10, 20, 30],\n",
    "             'max_features': ['auto'],\n",
    "             'class_weight' :['balanced']}\n",
    "clf_rf = GridSearchCV( RandomForestClassifier(), parameters, n_jobs = -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [10, 20, 30], 'max_features': ['auto'], 'class_weight': ['balanced']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf.fit(Xlr, ylr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame(clf_rf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98066473929179221"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf.score(Xtestlr, ytestlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score from learning dataset 0.999499116864\n",
      "Accuracy score from testing dataset 0.980664739292\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score\n",
    "\n",
    "# Print the accuracy from the test data.\n",
    "print('Accuracy score from learning dataset',accuracy_score(ylr, clf_rf.predict(Xlr) ))\n",
    "\n",
    "# Print the accuracy from the test data.\n",
    "print('Accuracy score from testing dataset',accuracy_score(ytestlr, clf_rf.predict(Xtestlr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learing dataset: \n",
      "\n",
      "         F1  Precision   Recall  Total\n",
      "0  1.000000    1.00000  1.00000  13630\n",
      "1  0.991875    1.00000  0.98388   3536\n",
      "2  0.999705    0.99941  1.00000  96633\n",
      "Test dataset: \n",
      "\n",
      "         F1  Precision    Recall  Total\n",
      "0  0.977991   0.998550  0.958261   5750\n",
      "1  0.692776   0.975490  0.537112   1482\n",
      "2  0.988902   0.978439  0.999591  41539\n"
     ]
    }
   ],
   "source": [
    "#Precision, Recall, F1\n",
    "print ('Learing dataset: \\n')\n",
    "prec, recall, f1, support = precision_recall_fscore_support(ylr, clf_rf.predict(Xlr))\n",
    "prf_lr = pd.DataFrame({'Precision': prec, 'Recall': recall, 'F1': f1, 'Total': support})\n",
    "print (prf_lr)\n",
    "\n",
    "print ('Test dataset: \\n')\n",
    "prec, recall, f1, support = precision_recall_fscore_support(ytestlr, clf_rf.predict(Xtestlr))\n",
    "\n",
    "prf_test = pd.DataFrame({'Precision': prec, 'Recall': recall, 'F1': f1, 'Total': support})\n",
    "print (prf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learing dataset: \n",
      "\n",
      "[[13630     0     0]\n",
      " [    0  3479    57]\n",
      " [    0     0 96633]]\n",
      "Test dataset: \n",
      "\n",
      "[[ 5510     6   234]\n",
      " [    5   796   681]\n",
      " [    3    14 41522]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "\n",
    "print ('Learing dataset: \\n')\n",
    "print (confusion_matrix(ylr, clf_rf.predict(Xlr)))\n",
    "\n",
    "print ('Test dataset: \\n')\n",
    "print (confusion_matrix(ytestlr, clf_rf.predict(Xtestlr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97861689594227819"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#F1 score\n",
    "f1_score(ytestlr, clf_rf.predict(Xtestlr), average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG Boost\n",
    "\n",
    " Re-using the learning set and test set created earlier using train_test_split (70% | 30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(Xlr, label = ylr)\n",
    "#clf_xg = xgb.xgboost()\n",
    "dtest = xgb.DMatrix(Xtestlr, label = ytestlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-merror:0.026594\ttrain-merror:0.025642\n",
      "[1]\teval-merror:0.02278\ttrain-merror:0.022144\n",
      "[2]\teval-merror:0.02073\ttrain-merror:0.020475\n",
      "[3]\teval-merror:0.01751\ttrain-merror:0.017223\n",
      "[4]\teval-merror:0.017264\ttrain-merror:0.017215\n",
      "[5]\teval-merror:0.016875\ttrain-merror:0.016301\n",
      "[6]\teval-merror:0.015932\ttrain-merror:0.015009\n",
      "[7]\teval-merror:0.014168\ttrain-merror:0.013445\n",
      "[8]\teval-merror:0.013328\ttrain-merror:0.012531\n",
      "[9]\teval-merror:0.012835\ttrain-merror:0.012215\n",
      "[10]\teval-merror:0.012548\ttrain-merror:0.011889\n",
      "[11]\teval-merror:0.012528\ttrain-merror:0.011854\n",
      "[12]\teval-merror:0.012528\ttrain-merror:0.011635\n",
      "[13]\teval-merror:0.012179\ttrain-merror:0.011327\n",
      "[14]\teval-merror:0.012179\ttrain-merror:0.011248\n"
     ]
    }
   ],
   "source": [
    "param = {'booster': 'gbtree','silent':0, 'objective':'multi:softmax', 'num_class' : 3 , 'eta':0.3}\n",
    "param['scale_pos_weight']= 0.176\n",
    "param['max_delta_step'] = 5\n",
    "param['min_samples_leaf'] = 15\n",
    "param['eval_metric'] = 'merror'\n",
    "\n",
    "evallist  = [(dtest,'eval'), (dtrain,'train')]\n",
    "\n",
    "num_round = 15\n",
    "bst = xgb.train( param, dtrain, num_round, evallist )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score from learning dataset 0.988752097997\n",
      "Accuracy score from testing dataset 0.987820631113\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score\n",
    "Xlr_xg = xgb.DMatrix(Xlr)\n",
    "Xtestlr_xg = xgb.DMatrix(Xtestlr)\n",
    "# Print the accuracy from the test data.\n",
    "print('Accuracy score from learning dataset',accuracy_score(ylr, bst.predict(Xlr_xg) ))\n",
    "\n",
    "# Print the accuracy from the test data.\n",
    "print('Accuracy score from testing dataset',accuracy_score(ytestlr, bst.predict(Xtestlr_xg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learing dataset: \n",
      "\n",
      "         F1  Precision    Recall  Total\n",
      "0  0.988258   0.994796  0.981805  13630\n",
      "1  0.839370   0.985883  0.730769   3536\n",
      "2  0.993553   0.987997  0.999172  96633\n",
      "Test dataset: \n",
      "\n",
      "         F1  Precision    Recall  Total\n",
      "0  0.987317   0.993137  0.981565   5750\n",
      "1  0.817439   0.965961  0.708502   1482\n",
      "2  0.993129   0.987667  0.998652  41539\n"
     ]
    }
   ],
   "source": [
    "#Precision, Recall, F1\n",
    "print ('Learing dataset: \\n')\n",
    "prec, recall, f1, support = precision_recall_fscore_support(ylr, bst.predict(Xlr_xg))\n",
    "prf_xg = pd.DataFrame({'Precision': prec, 'Recall': recall, 'F1': f1, 'Total': support})\n",
    "print (prf_xg)\n",
    "\n",
    "print ('Test dataset: \\n')\n",
    "prec, recall, f1, support = precision_recall_fscore_support(ytestlr, bst.predict(Xtestlr_xg))\n",
    "\n",
    "prf_test_xg = pd.DataFrame({'Precision': prec, 'Recall': recall, 'F1': f1, 'Total': support})\n",
    "print (prf_test_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learing dataset: \n",
      "\n",
      "[[13382    20   228]\n",
      " [    7  2584   945]\n",
      " [   63    17 96553]]\n",
      "Test dataset: \n",
      "\n",
      "[[ 5644     9    97]\n",
      " [   11  1050   421]\n",
      " [   28    28 41483]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "\n",
    "print ('Learing dataset: \\n')\n",
    "print (confusion_matrix(ylr, bst.predict(Xlr_xg)))\n",
    "\n",
    "print ('Test dataset: \\n')\n",
    "print (confusion_matrix(ytestlr, bst.predict(Xtestlr_xg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98710517452665747"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#F1 score\n",
    "f1_score(ytestlr, bst.predict(Xtestlr_xg), average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the best model in the 2012-2013 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loans_1213 = pd.read_csv('./LoanStats2012to2013.csv', skiprows=1, skipfooter=2, parse_dates = [15, 26, 45,47, 48], \n",
    "                        infer_datetime_format = True, engine = 'python', converters = {'int_rate' : p2f, 'revol_util' : p2f})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removing the attribute columns that have all null values. Only the attributes listed in the above DataDictionary will remain.\n",
    "loans_1213.dropna(axis = 1, how = 'all', inplace= True)\n",
    "\n",
    "#Since id values are not provided in the dataset, create id variable from the index.\n",
    "loans_1213['id'] = loans_1213.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188181"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_1213['term_mnths'] = loans_1213.term.str.strip('months').astype(int)\n",
    "len(loans_1213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143892"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering out the 36 months term loans\n",
    "loans_1213 = loans_1213[loans_1213.term_mnths == 36]\n",
    "len(loans_1213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dropping textual columns from the dataset\n",
    "loans_1213.drop(['emp_title', 'desc', 'title'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Marking loans that had at least one late payment\n",
    "loans_1213['late_fee_rec_indicator'] = (loans_1213.total_rec_late_fee > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loans_1213['loan_class'] = loans_1213.apply(classify_loan, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bad</th>\n",
       "      <td>18268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delinquent</th>\n",
       "      <td>2178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Good</th>\n",
       "      <td>123446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "loan_class        \n",
       "Bad          18268\n",
       "Delinquent    2178\n",
       "Good        123446"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(loans_1213.groupby('loan_class').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_datetime = datetime.strptime('2017-04-02', '%Y-%m-%d')\n",
    "# Issue date\n",
    "loans_1213['mo_sin_loan_funded'] = ((ref_datetime - loans_1213['issue_d']) / np.timedelta64(1, 'M')).astype(int)\n",
    "\n",
    "# Earliest credit line\n",
    "loans_1213['mo_sin_earliest_cr_line'] = ((ref_datetime - loans_1213['earliest_cr_line']) / np.timedelta64(1, 'M')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imputing missing values and converting to duration units\n",
    "#Last payment date is null when no installments have been made. Setting the nulls to May 2017, so that the calculated duration\n",
    "# field will have the value -1\n",
    "loans_1213['last_pymnt_d'] = loans_1213['last_pymnt_d'].fillna(pd.to_datetime('2017-05-02'))\n",
    "loans_1213['mo_sin_last_pymnt'] = ((ref_datetime - loans_1213['last_pymnt_d']) / np.timedelta64(1, 'M')).astype(int)\n",
    "\n",
    "#last_credit_pull_d\n",
    "#Setting the nulls to May 2017, so that the calculated duration field will have the value -1\n",
    "loans_1213['last_credit_pull_d'] = loans_1213['last_credit_pull_d'].fillna(pd.to_datetime('2017-05-02'))\n",
    "loans_1213['mo_sin_last_credit_pull'] = ((ref_datetime - loans_1213['last_credit_pull_d']) / np.timedelta64(1, 'M')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imputing missing values\n",
    "\n",
    "#mths_since_last_delinq\n",
    "# this will be null when the applicant has no prior delinquencies. Imputing these to -1 to set these apart.\n",
    "loans_1213['mths_since_last_delinq'] = loans_1213['mths_since_last_delinq'].fillna(-1)\n",
    "\n",
    "#mths_since_last_record\n",
    "# this will be null when the applicant has no public records. Imputing these to -1 to set these apart.\n",
    "loans_1213['mths_since_last_record'] = loans_1213['mths_since_last_record'].fillna(-1)\n",
    "\n",
    "#mths_since_last_major_derog\n",
    "# this will be null when the applicant has no previous derogatory records. Imputing these to -1 to set these apart.\n",
    "loans_1213['mths_since_last_major_derog'] = loans_1213['mths_since_last_major_derog'].fillna(-1)\n",
    "\n",
    "#mo_sin_old_il_acct\n",
    "# this will be null when the applicant has no bank installment accounts. Imputing these to -1 to set these apart.\n",
    "loans_1213['mo_sin_old_il_acct'] = loans_1213['mo_sin_old_il_acct'].fillna(-1)\n",
    "\n",
    "#mths_since_recent_bc_dlq\n",
    "# this will be null when there's no previous bankcard delinquency. Imputing these to -1 to set these apart.\n",
    "loans_1213['mths_since_recent_bc_dlq'] = loans_1213['mths_since_recent_bc_dlq'].fillna(-1)\n",
    "\n",
    "#mths_since_recent_inq\n",
    "# this will be null when there's no previous inquiries. Imputing these to -1 to set these apart.\n",
    "loans_1213['mths_since_recent_inq'] = loans_1213['mths_since_recent_inq'].fillna(-1)\n",
    "\n",
    "#mths_since_recent_revol_delinq\n",
    "# this will be null when there's no previous revolving account delinquencies. Imputing these to -1 to set these apart.\n",
    "loans_1213['mths_since_recent_revol_delinq'] = loans_1213['mths_since_recent_revol_delinq'].fillna(-1)\n",
    "\n",
    "#num_tl_120dpd_2m\n",
    "# this will be null when there's no accounts currently 120 days past due. Imputing these to 0.\n",
    "loans_1213['num_tl_120dpd_2m'] = loans_1213['num_tl_120dpd_2m'].fillna(0)\n",
    "\n",
    "#mths_since_recent_bc\n",
    "# this will be null when info not available. Imputing these to the median\n",
    "loans_1213['mths_since_recent_bc'] = loans_1213['mths_since_recent_bc'].fillna(loans_1213.mths_since_recent_bc.median())\n",
    "\n",
    "#bc_open_to_buy\n",
    "# this will be null when info is not available. Imputing these to the median\n",
    "loans_1213['bc_open_to_buy'] = loans_1213['bc_open_to_buy'].fillna(loans_1213.bc_open_to_buy.median())\n",
    "\n",
    "#bc_util\n",
    "# this will be null when info is not available. Imputing these to the median\n",
    "loans_1213['bc_util'] = loans_1213['bc_util'].fillna(loans_1213.bc_util.median())\n",
    "\n",
    "#avg_cur_bal\n",
    "# this will be null when info is not available. Imputing these to the median\n",
    "loans_1213['avg_cur_bal'] = loans_1213['avg_cur_bal'].fillna(loans_1213.avg_cur_bal.median())\n",
    "\n",
    "#percent_bc_gt_75\n",
    "# this will be null when info is not available. Imputing these to the median\n",
    "loans_1213['percent_bc_gt_75'] = loans_1213['percent_bc_gt_75'].fillna(loans_1213.percent_bc_gt_75.median())\n",
    "\n",
    "#revol_util\n",
    "# this will be null when info is not available. Imputing these to the median\n",
    "loans_1213['revol_util'] = loans_1213['revol_util'].fillna(loans_1213.revol_util.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop the now unused columns\n",
    "loans_1213.drop(['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert categorical features to One Hot Encode format using Pandas getDummies\n",
    "categorical_features = ['grade', 'sub_grade', 'home_ownership','emp_length', 'verification_status', 'pymnt_plan', 'purpose', 'addr_state',\n",
    "                       'initial_list_status']\n",
    "loans_1213_with_dummies = pd.get_dummies(columns = categorical_features, data= loans_1213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identifying X values - Removing all derived columns, date columns, id and constants\n",
    "X_1213 = loans_1213_with_dummies.drop(['term', 'zip_code','loan_status', 'application_type','id','term_mnths', 'late_fee_rec_indicator', \n",
    "                             'loan_class', 'next_pymnt_d' ], axis = 1)\n",
    "loans_1213_with_dummies['loan_class'] = loans_1213_with_dummies.loan_class.astype('category')\n",
    "Y_1213 = loans_1213_with_dummies.loan_class.cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57556, 201)\n",
      "(57556,)\n",
      "(86336, 201)\n",
      "(86336,)\n"
     ]
    }
   ],
   "source": [
    " # Split the data into a training and test set.\n",
    "Xlr_1213, Xtestlr_1213, ylr_1213, ytestlr_1213 = train_test_split(X_1213.values, Y_1213.values, test_size = 0.6, random_state = 5 )\n",
    "\n",
    "print(Xlr_1213.shape)\n",
    "print (ylr_1213.shape)\n",
    "\n",
    "print(Xtestlr_1213.shape)\n",
    "print(ytestlr_1213.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score :  0.974159099333\n",
      "F1 score :  0.975060440745\n",
      "\n",
      "          F1  Precision    Recall  Total\n",
      "0  0.907538   0.837017  0.991035  11043\n",
      "1  0.955626   0.991776  0.922018   1308\n",
      "2  0.985482   0.998751  0.972562  73985\n",
      "\n",
      " [[10944    10    89]\n",
      " [  101  1206     1]\n",
      " [ 2030     0 71955]]\n"
     ]
    }
   ],
   "source": [
    "#Run the test on the xgboost model\n",
    "\n",
    "# Accuracy score\n",
    "X_1213_xg = xgb.DMatrix(Xtestlr_1213)\n",
    "# Print the accuracy from the test data.\n",
    "print('Accuracy score : ',accuracy_score(ytestlr_1213, bst.predict(X_1213_xg) ))\n",
    "\n",
    "print('F1 score : ', f1_score(ytestlr_1213, bst.predict(X_1213_xg), average = 'weighted'))\n",
    "\n",
    "#Precision, Recall, F1\n",
    "\n",
    "prec, recall, f1, support = precision_recall_fscore_support(ytestlr_1213, bst.predict(X_1213_xg))\n",
    "\n",
    "prf_test_xg = pd.DataFrame({'Precision': prec, 'Recall': recall, 'F1': f1, 'Total': support})\n",
    "print ('\\n',prf_test_xg)\n",
    "\n",
    "print('\\n',confusion_matrix(ytestlr_1213, bst.predict(X_1213_xg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run the test on the RandomForest model\n",
    "\n",
    "# Accuracy score\n",
    "\n",
    "print('Accuracy score : ',accuracy_score(ytestlr_1213, clf_rf.predict(Xtestlr_1213) ))\n",
    "\n",
    "print('F1 score : ', f1_score(ytestlr_1213, clf_rf.predict(Xtestlr_1213), average = 'weighted'))\n",
    "\n",
    "#Precision, Recall, F1\n",
    "\n",
    "prec, recall, f1, support = precision_recall_fscore_support(ytestlr_1213, clf_rf.predict(Xtestlr_1213))\n",
    "\n",
    "prf_test_xg = pd.DataFrame({'Precision': prec, 'Recall': recall, 'F1': f1, 'Total': support})\n",
    "print ('\\n',prf_test_xg)\n",
    "\n",
    "print('\\n',confusion_matrix(ytestlr_1213, clf_rf.predict(Xtestlr_1213)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
